{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e624299",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "617e86ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (87812, 47)\n",
      "Columns: ['review_id', 'user_id', 'user_name', 'hotel_id', 'hotel_name', 'location', 'country', 'distance_center', 'hotel_rating_label', 'price', 'rating', 'review_text', 'review_title', 'review_date', 'traveler_type', 'stay_duration', 'sentiment_predicted', 'distance_center_km', 'hotel_rating', 'price_dzd', 'rating_normalized', 'review_date_parsed', 'review_year', 'review_month', 'review_day', 'review_day_of_week', 'location_area', 'location_city', 'stay_nights', 'review_text_clean', 'review_title_clean', 'review_length', 'review_word_count', 'traveler_type_encoded', 'sentiment_label', 'user_rating_count', 'user_avg_rating', 'user_rating_std', 'user_review_count', 'hotel_rating_count', 'hotel_avg_rating', 'hotel_rating_std', 'hotel_review_count', 'has_review_text', 'has_rating', 'has_date', 'is_complete']\n"
     ]
    }
   ],
   "source": [
    "file_path = \"../data/cleaned/data_cleaned_full.csv\"\n",
    "df_clean = pd.read_csv(file_path)\n",
    "print(f\"Dataset shape: {df_clean.shape}\")\n",
    "print(\"Columns:\", df_clean.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19e2a21",
   "metadata": {},
   "source": [
    "# 2. Load tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "511c3842",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9eaa183f44d495e990aaac153c798c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddb7659f51f5485690eff78652c368de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93e7523a1f7b4f55833b2501306c2502",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeb3e4f0fe2e46b3a2765bbd1a419c71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/695 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "camelbert_tokenizer = AutoTokenizer.from_pretrained(\"CAMeL-Lab/bert-base-arabic-camelbert-mix-sentiment\")\n",
    "arabert_tokenizer = AutoTokenizer.from_pretrained(\"Muhannedbsh/arabert-sentiment-model-MuhannedSh\")\n",
    "arat5_tokenizer = AutoTokenizer.from_pretrained(\"Noanihio/arat5v2-darja-sentiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0d2709",
   "metadata": {},
   "source": [
    "# 3. Function to tokenize and return token IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "075fccf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(tokenizer, text, max_length=128):\n",
    "    return tokenizer.encode(\n",
    "        str(text),\n",
    "        add_special_tokens=True, \n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0643780d",
   "metadata": {},
   "source": [
    "# 4. Apply tokenization for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c5e206d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All sentiment tokenizers loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# 1️⃣ CAMeLBERT sentiment tokenizer (3-class sentiment)\n",
    "camelbert_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"CAMeL-Lab/bert-base-arabic-camelbert-mix-sentiment\"\n",
    ")\n",
    "\n",
    "# 2️⃣ AraBERT sentence-transformer / sentiment tokenizer\n",
    "# (kathaem's version for classification tasks)\n",
    "arabert_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"kathaem/aubmindlab-arabertv02-base-sentence-transformer-xnli-ar\"\n",
    ")\n",
    "\n",
    "# 3️⃣ T5 sentiment tokenizer (Algerian Darja)\n",
    "arat5_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"Noanihio/arat5v2-darja-sentiment\"\n",
    ")\n",
    "\n",
    "print(\"✅ All sentiment tokenizers loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543f8285",
   "metadata": {},
   "source": [
    "# 5. Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8cfcafb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create token columns for each model\n",
    "df_clean[\"review_token_ids_arabert\"] = df_clean[\"review_text_clean\"].apply(\n",
    "    lambda x: tokenize_text(arabert_tokenizer, x)\n",
    ")\n",
    "\n",
    "df_clean[\"review_token_ids_camelbert\"] = df_clean[\"review_text_clean\"].apply(\n",
    "    lambda x: tokenize_text(camelbert_tokenizer, x)\n",
    ")\n",
    "\n",
    "df_clean[\"review_token_ids_arat5v2\"] = df_clean[\"review_text_clean\"].apply(\n",
    "    lambda x: tokenize_text(arat5_tokenizer, x)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f4ca4a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset saved\n"
     ]
    }
   ],
   "source": [
    "df_clean.to_csv(\"../data/tokenized/data_cleaned_full_with_tokens.csv\", index=False)\n",
    "print(f\"✅ Dataset saved\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
