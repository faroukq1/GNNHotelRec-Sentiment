{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "be1cc1c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/nbx/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "# Arabic NLP libraries\n",
    "import pyarabic.araby as araby\n",
    "from pyarabic.araby import strip_tashkeel, strip_tatweel, normalize_hamza, normalize_alef\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download NLTK Arabic stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d4d0f5ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (63257, 6)\n",
      "\n",
      "First few rows:\n",
      "   Unnamed: 0  rating  review_id   user_id   book_id  \\\n",
      "0           0       4  338670838   7878381  13431841   \n",
      "1           1       4   39428407   1775679   3554772   \n",
      "2           2       4   32159373   1304410   3554772   \n",
      "3           3       1  442326656  11333112   3554772   \n",
      "4           4       5   46492258    580165   3554772   \n",
      "\n",
      "                                         review_text  \n",
      "0   \"عزازيل الذي صنعناه ،الكامن في أنفسنا\" يذكرني...  \n",
      "1   من أمتع ما قرأت من روايات بلا شك. وحول الشك ت...  \n",
      "2   رواية تتخذ من التاريخ ،جوًا لها اختار المؤلف ...  \n",
      "3   إني أقدّر هذه الرواية كثيرا، لسبب مختلف عن أس...  \n",
      "4   الكاهن الذي أطلق على نفسه اسم هيبا تيمنا بالع...  \n",
      "\n",
      "Rating distribution:\n",
      "rating\n",
      "1     2939\n",
      "2     5285\n",
      "3    12201\n",
      "4    19054\n",
      "5    23778\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load your CSV file\n",
    "df = pd.read_csv('../data/row/labr_row.csv')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "print(f\"\\nRating distribution:\")\n",
    "print(df['rating'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c790280",
   "metadata": {},
   "source": [
    "# ARABIC TEXT CLEANING FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bc6bcdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_diacritics(text):\n",
    "    \"\"\"Remove Arabic diacritics (tashkeel)\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    return strip_tashkeel(text)\n",
    "\n",
    "def remove_tatweel(text):\n",
    "    \"\"\"Remove tatweel (ـ)\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    return strip_tatweel(text)\n",
    "\n",
    "def normalize_arabic(text):\n",
    "    \"\"\"Normalize Arabic letters\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    # Normalize different forms of Alef to ا\n",
    "    text = re.sub(r'[إأآٱ]', 'ا', text)\n",
    "    # Normalize different forms of Hamza\n",
    "    text = re.sub(r'[ؤئ]', 'ء', text)\n",
    "    # Replace ة with ه at the end of words\n",
    "    text = re.sub(r'ة([^\\w]|$)', r'ه\\1', text)\n",
    "    # Normalize ى to ي\n",
    "    text = re.sub(r'ى', 'ي', text)\n",
    "    return text\n",
    "\n",
    "def remove_repeating_chars(text):\n",
    "    \"\"\"Remove repeated characters (e.g., هههههه -> هه)\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    # Keep max 2 repetitions\n",
    "    return re.sub(r'(.)\\1{2,}', r'\\1\\1', text)\n",
    "\n",
    "def remove_english_text(text):\n",
    "    \"\"\"Remove English characters\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    return re.sub(r'[a-zA-Z]+', '', text)\n",
    "\n",
    "def remove_urls(text):\n",
    "    \"\"\"Remove URLs\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    return re.sub(r'http\\S+|www.\\S+', '', text)\n",
    "\n",
    "def remove_emails(text):\n",
    "    \"\"\"Remove email addresses\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    return re.sub(r'\\S+@\\S+', '', text)\n",
    "\n",
    "def remove_mentions_hashtags(text):\n",
    "    \"\"\"Remove mentions and hashtags\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "    return text\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    \"\"\"Remove Arabic and English punctuation\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    arabic_punctuation = '''`÷×؛<>_()*&^%][ـ،/:\"؟.,'{}~¦+|!\"…\"–ـ'''\n",
    "    english_punctuation = string.punctuation\n",
    "    all_punctuation = arabic_punctuation + english_punctuation\n",
    "    translator = str.maketrans('', '', all_punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "def remove_numbers(text):\n",
    "    \"\"\"Remove Arabic and English numbers\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    # Remove English numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Remove Arabic-Indic numbers\n",
    "    text = re.sub(r'[٠-٩]+', '', text)\n",
    "    return text\n",
    "\n",
    "def remove_extra_whitespace(text):\n",
    "    \"\"\"Remove extra whitespaces\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "def remove_emojis(text):\n",
    "    \"\"\"Remove emojis\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "def remove_arabic_stopwords(text):\n",
    "    \"\"\"Remove Arabic stopwords\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    arabic_stopwords = set(stopwords.words('arabic'))\n",
    "    \n",
    "    # Add common Arabic stopwords that might be missed\n",
    "    additional_stopwords = {\n",
    "        'من', 'الى', 'إلى', 'عن', 'على', 'في', 'حتى', 'أو', 'و', 'ف', 'ثم',\n",
    "        'لكن', 'كان', 'هذا', 'هذه', 'ذلك', 'التي', 'الذي', 'كل', 'بعض', 'هل',\n",
    "        'لا', 'نعم', 'إن', 'أن', 'كما', 'لقد', 'قد', 'ليس', 'غير', 'بل', 'لم'\n",
    "    }\n",
    "    arabic_stopwords.update(additional_stopwords)\n",
    "    \n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word not in arabic_stopwords]\n",
    "    return ' '.join(filtered_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8dc8be",
   "metadata": {},
   "source": [
    "# COMPREHENSIVE PREPROCESSING PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "26c127bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_arabic_text(text, remove_stopwords=False, keep_punctuation=False):\n",
    "    \"\"\"\n",
    "    Complete preprocessing pipeline for Arabic text\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    text : str\n",
    "        Input Arabic text\n",
    "    remove_stopwords : bool\n",
    "        Whether to remove stopwords (default: False for BERT models)\n",
    "    keep_punctuation : bool\n",
    "        Whether to keep punctuation (default: False)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    str : Preprocessed text\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Step 1: Remove diacritics\n",
    "    text = remove_diacritics(text)\n",
    "    \n",
    "    # Step 2: Remove tatweel\n",
    "    text = remove_tatweel(text)\n",
    "    \n",
    "    # Step 3: Normalize Arabic characters\n",
    "    text = normalize_arabic(text)\n",
    "    \n",
    "    # Step 4: Remove URLs, emails, mentions, hashtags\n",
    "    text = remove_urls(text)\n",
    "    text = remove_emails(text)\n",
    "    text = remove_mentions_hashtags(text)\n",
    "    \n",
    "    # Step 5: Remove emojis\n",
    "    text = remove_emojis(text)\n",
    "    \n",
    "    # Step 6: Remove English text\n",
    "    text = remove_english_text(text)\n",
    "    \n",
    "    # Step 7: Remove numbers\n",
    "    text = remove_numbers(text)\n",
    "    \n",
    "    # Step 8: Remove repeating characters\n",
    "    text = remove_repeating_chars(text)\n",
    "    \n",
    "    # Step 9: Remove punctuation (optional)\n",
    "    if not keep_punctuation:\n",
    "        text = remove_punctuation(text)\n",
    "    \n",
    "    # Step 10: Remove stopwords (optional - usually not needed for BERT)\n",
    "    if remove_stopwords:\n",
    "        text = remove_arabic_stopwords(text)\n",
    "    \n",
    "    # Step 11: Remove extra whitespace\n",
    "    text = remove_extra_whitespace(text)\n",
    "    \n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d352b47f",
   "metadata": {},
   "source": [
    "# APPLY PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b5c6d325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PREPROCESSING ARABIC REVIEWS\n",
      "================================================================================\n",
      "\n",
      "Applying preprocessing pipeline...\n",
      "\n",
      "Original dataset size: 63257\n",
      "Cleaned dataset size: 63254\n",
      "Removed 3 empty reviews\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PREPROCESSING ARABIC REVIEWS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create a copy of the dataframe\n",
    "df_clean = df.copy()\n",
    "\n",
    "# Apply preprocessing\n",
    "print(\"\\nApplying preprocessing pipeline...\")\n",
    "df_clean['review_text_clean'] = df_clean['review_text'].apply(\n",
    "    lambda x: preprocess_arabic_text(x, remove_stopwords=False, keep_punctuation=True)\n",
    ")\n",
    "\n",
    "# Remove empty reviews after preprocessing\n",
    "df_clean = df_clean[df_clean['review_text_clean'].str.strip() != '']\n",
    "\n",
    "print(f\"\\nOriginal dataset size: {len(df)}\")\n",
    "print(f\"Cleaned dataset size: {len(df_clean)}\")\n",
    "print(f\"Removed {len(df) - len(df_clean)} empty reviews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24afa5c",
   "metadata": {},
   "source": [
    "# DATA QUALITY CHECKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "40955fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DATA QUALITY CHECKS\n",
      "================================================================================\n",
      "\n",
      "Text length statistics:\n",
      "count    63254.000000\n",
      "mean        61.875344\n",
      "std        107.856722\n",
      "min          1.000000\n",
      "25%         14.000000\n",
      "50%         31.000000\n",
      "75%         67.000000\n",
      "max       3403.000000\n",
      "Name: text_length, dtype: float64\n",
      "\n",
      "Removed reviews with less than 3 words\n",
      "Final dataset size: 61695\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA QUALITY CHECKS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check text lengths\n",
    "df_clean['text_length'] = df_clean['review_text_clean'].str.split().str.len()\n",
    "\n",
    "print(\"\\nText length statistics:\")\n",
    "print(df_clean['text_length'].describe())\n",
    "\n",
    "# Remove very short reviews (less than 3 words)\n",
    "min_words = 3\n",
    "df_clean = df_clean[df_clean['text_length'] >= min_words]\n",
    "print(f\"\\nRemoved reviews with less than {min_words} words\")\n",
    "print(f\"Final dataset size: {len(df_clean)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d857590",
   "metadata": {},
   "source": [
    "# PREVIEW RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b63b9cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PREPROCESSING EXAMPLES\n",
      "================================================================================\n",
      "\n",
      "--- Example 1 ---\n",
      "Rating: 4\n",
      "\n",
      "Original (618 chars):\n",
      " \"عزازيل الذي صنعناه ،الكامن في أنفسنا\" يذكرني يوسف زيدان بــ بورخس في استخدامه لحيلته الفنية،وخداع القاريء بأن الرواية ترجمة لمخطوط قديم. الهوامش المخترعة و اختلاق وجود مترجـِم عاد بي إلى بورخس و هوا...\n",
      "\n",
      "Cleaned (593 chars):\n",
      "\"عزازيل الذي صنعناه ،الكامن في انفسنا\" يذكرني يوسف زيدان ب بورخس في استخدامه لحيلته الفنيه،وخداع القاريء بان الروايه ترجمه لمخطوط قديم. الهوامش المخترعه و اختلاق وجود مترجم عاد بي الي بورخس و هوامشه و\n",
      "\n",
      "--- Example 2 ---\n",
      "Rating: 4\n",
      "\n",
      "Original (86 chars):\n",
      " من أمتع ما قرأت من روايات بلا شك. وحول الشك تدندن (عزازيل) بلا هوادة. أحمد الديب 2008...\n",
      "\n",
      "Cleaned (80 chars):\n",
      "من امتع ما قرات من روايات بلا شك. وحول الشك تدندن (عزازيل) بلا هواده. احمد الديب\n",
      "\n",
      "--- Example 3 ---\n",
      "Rating: 4\n",
      "\n",
      "Original (193 chars):\n",
      " رواية تتخذ من التاريخ ،جوًا لها اختار المؤلف فترة تاريخية ندر من يتناولها روائيًا. مكتوبة بدقة وإتقان وجمال.من أروع ما يمكن أن تقرأ من الروايات التاريخية. تركز على الإنسان.صانع المعنى ومدمره ....\n",
      "\n",
      "Cleaned (190 chars):\n",
      "روايه تتخذ من التاريخ ،جوا لها اختار المءلف فتره تاريخيه ندر من يتناولها رواءيا. مكتوبه بدقه واتقان وجمال.من اروع ما يمكن ان تقرا من الروايات التاريخيه. تركز علي الانسان.صانع المعني ومدمره .\n",
      "\n",
      "--- Example 4 ---\n",
      "Rating: 1\n",
      "\n",
      "Original (2786 chars):\n",
      " إني أقدّر هذه الرواية كثيرا، لسبب مختلف عن أسباب الآخرين، ألا وهو أنها علمتني درسا قيما، حتى وإن كانت العبرة قد أتت بعد فوات الأوان، وذلك عهدنا بدروس الحياة. علمتني الرواية أن الصديق الذي يشجعك على ق...\n",
      "\n",
      "Cleaned (2773 chars):\n",
      "اني اقدر هذه الروايه كثيرا، لسبب مختلف عن اسباب الاخرين، الا وهو انها علمتني درسا قيما، حتي وان كانت العبره قد اتت بعد فوات الاوان، وذلك عهدنا بدروس الحياه. علمتني الروايه ان الصديق الذي يشجعك علي قرا\n",
      "\n",
      "--- Example 5 ---\n",
      "Rating: 5\n",
      "\n",
      "Original (729 chars):\n",
      " الكاهن الذي أطلق على نفسه اسم هيبا تيمنا بالعالمة الوثنية هيباتيا فر من قريته بعد أن وشت أمه المسيحية بوالده الوثني فقتل وتزوجت قاتله. ذهب باحثا عن الدين والعلوم الطبية. من خلال رحلته نتعرف على تاريخ...\n",
      "\n",
      "Cleaned (727 chars):\n",
      "الكاهن الذي اطلق علي نفسه اسم هيبا تيمنا بالعالمه الوثنيه هيباتيا فر من قريته بعد ان وشت امه المسيحيه بوالده الوثني فقتل وتزوجت قاتله. ذهب باحثا عن الدين والعلوم الطبيه. من خلال رحلته نتعرف علي تاريخ \n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PREPROCESSING EXAMPLES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Show some examples\n",
    "for idx in df_clean.head(5).index:\n",
    "    print(f\"\\n--- Example {idx + 1} ---\")\n",
    "    print(f\"Rating: {df_clean.loc[idx, 'rating']}\")\n",
    "    print(f\"\\nOriginal ({len(df_clean.loc[idx, 'review_text'])} chars):\")\n",
    "    print(df_clean.loc[idx, 'review_text'][:200] + \"...\")\n",
    "    print(f\"\\nCleaned ({len(df_clean.loc[idx, 'review_text_clean'])} chars):\")\n",
    "    print(df_clean.loc[idx, 'review_text_clean'][:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3e3dcaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.to_csv('../data/processed/labr_cleaned.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
